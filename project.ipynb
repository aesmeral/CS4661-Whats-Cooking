{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a714500-4b85-43d9-9b88-922f22ac783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aesmeral/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, json, os, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define stopwords\n",
    "cooking_stopwords = [\"recipe\", \"cook\", \"cooking\", \"bake\", \"boil\", \"grill\", \"saute\", \"roast\", \"simmer\", \"fry\", \"stir\", \"season\", \"dish\", \"plate\", \"meal\", \"serve\"]\n",
    "standard_stopwords = set(stopwords.words(\"english\"))\n",
    "stop_words = standard_stopwords.union(cooking_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ef7255-fb8d-4210-b173-89e49e5f3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CookingTest.py', 'project.ipynb', 'results.csv', 'train.json', 'sample_submission.csv', '.git', 'README.md', 'test.json', '.ipynb_checkpoints', 'CookingTest.py:Zone.Identifier', 'output.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3263d7d-5eaf-472f-baf5-8c1902fe329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_json('./train.json')\n",
    "testing_data = pd.read_json('./test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7005c680-eca5-4c74-b640-fb1767b35cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'cuisine', 'ingredients'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(training_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5271ed9f-f691-4eb3-a627-8a25c352f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              greek\n",
      "1        southern_us\n",
      "2           filipino\n",
      "3             indian\n",
      "4             indian\n",
      "            ...     \n",
      "39769          irish\n",
      "39770        italian\n",
      "39771          irish\n",
      "39772        chinese\n",
      "39773        mexican\n",
      "Name: cuisine, Length: 39774, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(training_data['cuisine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9a8a05-cf36-43fe-a55c-89eb8a9da54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [romaine lettuce, black olives, grape tomatoes...\n",
      "1        [plain flour, ground pepper, salt, tomatoes, g...\n",
      "2        [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
      "3                      [water, vegetable oil, wheat, salt]\n",
      "4        [black pepper, shallots, cornflour, cayenne pe...\n",
      "                               ...                        \n",
      "39769    [light brown sugar, granulated sugar, butter, ...\n",
      "39770    [KRAFT Zesty Italian Dressing, purple onion, b...\n",
      "39771    [eggs, citrus fruit, raisins, sourdough starte...\n",
      "39772    [boneless chicken skinless thigh, minced garli...\n",
      "39773    [green chile, jalapeno chilies, onions, ground...\n",
      "Name: ingredients, Length: 39774, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(training_data['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d672a1ce-f475-4ed0-b6d3-a13a4789b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently preprocess data\n",
    "def scrubbed_ingredient(ingredient):\n",
    "    scrubbed = re.sub(r'[^a-zA-Z ]', '', ingredient).lower()\n",
    "    scrubbed = re.sub(r'oz|crushed|crumbles|ground|minced|powder|chopped|sliced|boneless|skinless|fresh|frozen|homemade|instance|kraft|large|lean|lowfat|small|smoke|vegan', '', scrubbed).lstrip()\n",
    "    scrubbed = \"_\".join([stemmer.stem(word) for word in scrubbed.split() if word not in stop_words])\n",
    "    return scrubbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2873111-77e0-4eb0-af30-31253a0fb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently build ingredient vocabulary\n",
    "ingredients_set = set()\n",
    "for ingredients_list in training_data['ingredients']:\n",
    "    scrubbed_ingredients = [scrubbed_ingredient(ingredient) for ingredient in ingredients_list]\n",
    "    scrubbed_ingredients = [word for word in scrubbed_ingredients if len(word) >= 1]\n",
    "    ingredients_set.update(scrubbed_ingredients)\n",
    "\n",
    "sorted_set = sorted(ingredients_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99405fd-862d-4ecf-aa68-7f33da561b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2500\n",
      "5000\n",
      "7500\n",
      "10000\n",
      "12500\n",
      "15000\n",
      "17500\n",
      "20000\n",
      "22500\n",
      "25000\n",
      "27500\n",
      "30000\n",
      "32500\n",
      "35000\n",
      "37500\n"
     ]
    }
   ],
   "source": [
    "# Create a sparse matrix\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=sorted_set)\n",
    "batch_size = 2500  # Adjust the batch size as needed\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(0, len(training_data), batch_size):\n",
    "    batch = training_data['ingredients'].iloc[i:i+batch_size]\n",
    "    scrubbed_ingredients = [\" \".join([scrubbed_ingredient(ingredient) for ingredient in row]) for row in batch]\n",
    "    X = vectorizer.transform(scrubbed_ingredients)\n",
    "    result.append(X)\n",
    "    print(i)\n",
    "\n",
    "# Create a sparse DataFrame\n",
    "df = pd.DataFrame.sparse.from_spmatrix(vstack(result))\n",
    "df.columns = sorted_set\n",
    "df['cuisine'] = training_data['cuisine']\n",
    "df['id'] = training_data['id']\n",
    "\n",
    "df.head(100).to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73986b7-519e-45f1-b7f1-d9384a82757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df.iloc[:, :-2]\n",
    "y = df['cuisine']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e48dab2-9f4f-4307-962d-96c152303ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "model = LogisticRegression(multi_class='ovr', solver='sag', max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00250f14-2701-47d5-a75a-d2d1f2f2a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787806411062225\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67de864-231a-4829-a185-5c8735723543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             actual    prediction\n",
      "18708       italian       italian\n",
      "11518  cajun_creole  cajun_creole\n",
      "3939        chinese       chinese\n",
      "27897   southern_us        french\n",
      "13952       italian       italian\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['actual'] = y_test\n",
    "results['prediction'] = y_predict\n",
    "\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d90d346-b687-4799-bd32-65a4d5d06ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7955, 2)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)\n",
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ae0f3-e447-4d57-b18c-7dd3d92df933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
